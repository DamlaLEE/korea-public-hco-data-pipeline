import os
from datetime import datetime
import pandas as pd
import re
from collections import defaultdict

import boto3
from botocore.exceptions import NoCredentialsError

# Load and combine multiple files from a folder
def load_and_merge_files(
    folder_path,
    file_type="xlsx",
    max_files=None,
    sort_by_time=False
):
    """
    Load and merge multiple xlsx or csv files from a folder.

    Parameters:
    - folder_path (str): directory path containing the files
    - file_type (str): 'xlsx' or 'csv'
    - max_files (int or None): number of recent files to load, or all if None
    - sort_by_time (bool): if True, load recent files based on modified time

    Returns:
    - pd.DataFrame or None
    """

    if file_type not in ("xlsx", "csv"):
        raise ValueError("Only 'xlsx' and 'csv' file types are supported.")

    # 1. List matching files
    all_files = [
        os.path.join(folder_path, f)
        for f in os.listdir(folder_path)
        if f.endswith(f".{file_type}")
    ]

    if sort_by_time:
        all_files = sorted(all_files, key=os.path.getmtime, reverse=True)

    if max_files is not None:
        all_files = all_files[:max_files]

    print(f"üìÅ Found {len(all_files)} {file_type.upper()} file(s) to load:")

    for f in all_files:
        print(" -", os.path.basename(f))

    # 2. Read files
    df_list = []
    for file in all_files:
        try:
            if file_type == "xlsx":
                df = pd.read_excel(file)
            else:
                df = pd.read_csv(file)

            df["source_file"] = os.path.basename(file)
            df_list.append(df)
        except Exception as e:
            print(f"‚ùå Failed to read {file}: {e}")

    # 3. Merge
    if df_list:
        merged_df = pd.concat(df_list, ignore_index=True)
        print(f"‚úÖ Merged shape: {merged_df.shape}")
        return merged_df
    else:
        print("‚ùå No files were merged.")
        return None

# Extract numeric doctor data from text fields
def extract_doctor_counts(text):
    try:
        doctor = int(re.search(r"ÏùòÏÇ¨\s*:\s*(\d+)", text).group(1))
    except:
        doctor = None
    try:
        dentist = int(re.search(r"ÏπòÍ≥ºÏùòÏÇ¨\s*:\s*(\d+)", text).group(1))
    except:
        dentist = None
    try:
        korean_med = int(re.search(r"ÌïúÏùòÏÇ¨\s*:\s*(\d+)", text).group(1))
    except:
        korean_med = None
    return pd.Series([doctor, dentist, korean_med])

# Extract metadata (e.g., dept) from filename
def seperate_data(dataframe, column_name_new, column_name_raw, num):
    dataframe[column_name_new] = dataframe[column_name_raw].apply(
            lambda x: x.split("_")[num-1] if len(x.split("_")) >= num else None)

# Extract province/city from full address
def extract_region_info(address):
    try:
        parts = address.split()
        province = parts[0] if len(parts) > 0 else None
        city = parts[1] if len(parts) > 1 else None
    except:
        province, city = None, None
    return pd.Series([province, city])

# Analyze top hospitals by medical staff size
def get_top_hospitals_by_staff(df, category, top_n):
    filtered_df = df[df['category'] == category]
    filtered_df = filtered_df.sort_values(by='total_medical_staff', ascending=False)
    return filtered_df.head(top_n)

# Load final_dataset to S3
def upload_to_s3(file_path, bucket_name, s3_path):
    s3 = boto3.client('s3')  # Assumes local AWS credentials are set
    try:
        s3.upload_file(file_path, bucket_name, s3_path)
        print(f"‚úÖ Upload successful: s3://{bucket_name}/{s3_path}")
    except FileNotFoundError:
        print("‚ùå File not found.")
    except NoCredentialsError:
        print("‚ùå AWS credentials not configured in your environment.")